{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankitk108/Ankitk108/blob/main/docs/tutorials/quantum_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLOXFOT5Q40E"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iiQkM5ZgQ8r2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6331ZSsQGY3"
      },
      "source": [
        "# Quantum data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Jcnb8bQQyd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/quantum/tutorials/quantum_data\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/quantum/blob/master/docs/tutorials/quantum_data.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/quantum/blob/master/docs/tutorials/quantum_data.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/quantum/docs/tutorials/quantum_data.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2HoEn9BEWfn"
      },
      "source": [
        "Building off of the comparisons made in the [MNIST](https://www.tensorflow.org/quantum/tutorials/mnist) tutorial, this tutorial explores the recent work of [Huang et al.](https://arxiv.org/abs/2011.01938) that shows how different datasets affect performance comparisons. In the work, the authors seek to understand how and when classical machine learning models can learn as well as (or better than) quantum models. The work also showcases an empirical performance separation between classical and quantum machine learning model via a carefully crafted dataset. You will:\n",
        "\n",
        "1.   Prepare a reduced dimension Fashion-MNIST dataset.\n",
        "2.   Use quantum circuits to re-label the dataset and compute Projected Quantum Kernel features (PQK).\n",
        "3.   Train a classical neural network on the re-labeled dataset and compare the performance with a model that has access to the PQK features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQvswYv7LAaU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Y5vLL9K_Ai",
        "outputId": "b531c8ff-cead-498b-9f25-705cb2be989b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tensorflow-quantum==0.7.3\n",
            "  Downloading tensorflow_quantum-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.73.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting cirq-core==1.3.0 (from tensorflow-quantum==0.7.3)\n",
            "  Downloading cirq_core-1.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting cirq-google==1.3.0 (from tensorflow-quantum==0.7.3)\n",
            "  Downloading cirq_google-1.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sympy==1.12 (from tensorflow-quantum==0.7.3)\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting duet~=0.2.8 (from cirq-core==1.3.0->tensorflow-quantum==0.7.3)\n",
            "  Downloading duet-0.2.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (3.10.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.11/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (3.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2.2.2)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.11/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (4.67.1)\n",
            "Requirement already satisfied: google-api-core>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (2.25.1)\n",
            "Requirement already satisfied: proto-plus>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.26.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy==1.12->tensorflow-quantum==0.7.3) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core>=1.14.0->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2025.2)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.1)\n",
            "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_quantum-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cirq_core-1.3.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cirq_google-1.3.0-py3-none-any.whl (598 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.8/598.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duet-0.2.9-py3-none-any.whl (29 kB)\n",
            "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, sympy, protobuf, numpy, keras, duet, ml-dtypes, grpcio-status, tensorboard, cirq-core, tensorflow, cirq-google, tensorflow-quantum\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.71.0\n",
            "    Uninstalling grpcio-status-1.71.0:\n",
            "      Successfully uninstalled grpcio-status-1.71.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.0 tensorflow-quantum==0.7.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ql5PW-ACO0J"
      },
      "outputs": [],
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTKfetslL5eE"
      },
      "outputs": [],
      "source": [
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "# visualization tools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCOHgdILONs-"
      },
      "source": [
        "## 1. Data preparation\n",
        "\n",
        "You will begin by preparing the fashion-MNIST dataset for running on a quantum computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDl61cN6WPDk"
      },
      "source": [
        "### 1.1 Download fashion-MNIST\n",
        "\n",
        "The first step is to get the traditional fashion-mnist dataset. This can be done using the `tf.keras.datasets` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTKmzeH3MBvR"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test,\n",
        "                     y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq3eeFv2PyQz"
      },
      "source": [
        "Filter the dataset to keep just the T-shirts/tops and dresses, remove the other classes. At the same time convert the label, `y`, to boolean: True for 0 and False for 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmprnNbDP4Z6"
      },
      "outputs": [],
      "source": [
        "def filter_03(x, y):\n",
        "    keep = (y == 0) | (y == 3)\n",
        "    x, y = x[keep], y[keep]\n",
        "    y = y == 0\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KycvXPllQH-t"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = filter_03(x_train, y_train)\n",
        "x_test, y_test = filter_03(x_test, y_test)\n",
        "\n",
        "print(\"Number of filtered training examples:\", len(x_train))\n",
        "print(\"Number of filtered test examples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-2Fx9E1O63h"
      },
      "outputs": [],
      "source": [
        "print(y_train[0])\n",
        "\n",
        "plt.imshow(x_train[0, :, :])\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ASbMvu6SFST"
      },
      "source": [
        "### 1.2 Downscale the images\n",
        "\n",
        "Just like the MNIST example, you will need to downscale these images in order to be within the boundaries for current quantum computers. This time however you will use a PCA transformation to reduce the dimensions instead of a `tf.image.resize` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_EvK2kJPKDk"
      },
      "outputs": [],
      "source": [
        "def truncate_x(x_train, x_test, n_components=10):\n",
        "    \"\"\"Perform PCA on image dataset keeping the top `n_components` components.\"\"\"\n",
        "    n_points_train = tf.gather(tf.shape(x_train), 0)\n",
        "    n_points_test = tf.gather(tf.shape(x_test), 0)\n",
        "\n",
        "    # Flatten to 1D\n",
        "    x_train = tf.reshape(x_train, [n_points_train, -1])\n",
        "    x_test = tf.reshape(x_test, [n_points_test, -1])\n",
        "\n",
        "    # Normalize.\n",
        "    feature_mean = tf.reduce_mean(x_train, axis=0)\n",
        "    x_train_normalized = x_train - feature_mean\n",
        "    x_test_normalized = x_test - feature_mean\n",
        "\n",
        "    # Truncate.\n",
        "    e_values, e_vectors = tf.linalg.eigh(\n",
        "        tf.einsum('ji,jk->ik', x_train_normalized, x_train_normalized))\n",
        "    return tf.einsum('ij,jk->ik', x_train_normalized, e_vectors[:,-n_components:]), \\\n",
        "      tf.einsum('ij,jk->ik', x_test_normalized, e_vectors[:, -n_components:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WhtP5RRkYSI"
      },
      "outputs": [],
      "source": [
        "DATASET_DIM = 10\n",
        "x_train, x_test = truncate_x(x_train, x_test, n_components=DATASET_DIM)\n",
        "print(f'New datapoint dimension:', len(x_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAEeE50FS9G"
      },
      "source": [
        "The last step is to reduce the size of the dataset to just 1000 training datapoints and 200 testing datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMxlW2kZDtvn"
      },
      "outputs": [],
      "source": [
        "N_TRAIN = 1000\n",
        "N_TEST = 200\n",
        "x_train, x_test = x_train[:N_TRAIN], x_test[:N_TEST]\n",
        "y_train, y_test = y_train[:N_TRAIN], y_test[:N_TEST]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7vqUjDMGF2S"
      },
      "outputs": [],
      "source": [
        "print(\"New number of training examples:\", len(x_train))\n",
        "print(\"New number of test examples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-26obVJtHQne"
      },
      "source": [
        "## 2. Relabeling and computing PQK features\n",
        "\n",
        "You will now prepare a \"stilted\" quantum dataset by incorporating quantum components and re-labeling the truncated fashion-MNIST dataset you've created above. In order to get the most seperation between quantum and classical methods, you will first prepare the PQK features and then relabel outputs based on their values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJEK8CwKWgC2"
      },
      "source": [
        "### 2.1 Quantum encoding and PQK features\n",
        "You will create a new set of features, based on `x_train`, `y_train`, `x_test` and `y_test` that is defined to be the 1-RDM on all qubits of:\n",
        "\n",
        "$V(x_{\\text{train}} / n_{\\text{trotter}}) ^ {n_{\\text{trotter}}} U_{\\text{1qb}} | 0 \\rangle$\n",
        "\n",
        "Where $U_\\text{1qb}$ is a wall of single qubit rotations and $V(\\hat{\\theta}) = e^{-i\\sum_i \\hat{\\theta_i} (X_i X_{i+1} + Y_i Y_{i+1} + Z_i Z_{i+1})}$\n",
        "\n",
        "First, you can generate the wall of single qubit rotations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVTlHdGvEuaT"
      },
      "outputs": [],
      "source": [
        "def single_qubit_wall(qubits, rotations):\n",
        "    \"\"\"Prepare a single qubit X,Y,Z rotation wall on `qubits`.\"\"\"\n",
        "    wall_circuit = cirq.Circuit()\n",
        "    for i, qubit in enumerate(qubits):\n",
        "        for j, gate in enumerate([cirq.X, cirq.Y, cirq.Z]):\n",
        "            wall_circuit.append(gate(qubit)**rotations[i][j])\n",
        "\n",
        "    return wall_circuit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCfFcs-nGFH5"
      },
      "source": [
        "You can quickly verify this works by looking at the circuit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfJkWj88Fqwl"
      },
      "outputs": [],
      "source": [
        "SVGCircuit(\n",
        "    single_qubit_wall(cirq.GridQubit.rect(1, 4),\n",
        "                      np.random.uniform(size=(4, 3))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPniCQWnHGXz"
      },
      "source": [
        "Next you can prepare $V(\\hat{\\theta})$ with the help of `tfq.util.exponential` which can exponentiate any commuting `cirq.PauliSum` objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w2em6c0HOIO"
      },
      "outputs": [],
      "source": [
        "def v_theta(qubits):\n",
        "    \"\"\"Prepares a circuit that generates V(\\theta).\"\"\"\n",
        "    ref_paulis = [\n",
        "        cirq.X(q0) * cirq.X(q1) + \\\n",
        "        cirq.Y(q0) * cirq.Y(q1) + \\\n",
        "        cirq.Z(q0) * cirq.Z(q1) for q0, q1 in zip(qubits, qubits[1:])\n",
        "    ]\n",
        "    exp_symbols = list(sympy.symbols('ref_0:' + str(len(ref_paulis))))\n",
        "    return tfq.util.exponential(ref_paulis, exp_symbols), exp_symbols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo6ArnnqIkTL"
      },
      "source": [
        "This circuit might be a little bit harder to verify by looking at, but you can still examine a two qubit case to see what is happening:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7YIeOrzJDlT"
      },
      "outputs": [],
      "source": [
        "test_circuit, test_symbols = v_theta(cirq.GridQubit.rect(1, 2))\n",
        "print(f'Symbols found in circuit:{test_symbols}')\n",
        "SVGCircuit(test_circuit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN8oWtEXJXj-"
      },
      "source": [
        "Now you have all the building blocks you need to put your full encoding circuits together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LReAUF6CSwn5"
      },
      "outputs": [],
      "source": [
        "def prepare_pqk_circuits(qubits, classical_source, n_trotter=10):\n",
        "    \"\"\"Prepare the pqk feature circuits around a dataset.\"\"\"\n",
        "    n_qubits = len(qubits)\n",
        "    n_points = len(classical_source)\n",
        "\n",
        "    # Prepare random single qubit rotation wall.\n",
        "    random_rots = np.random.uniform(-2, 2, size=(n_qubits, 3))\n",
        "    initial_U = single_qubit_wall(qubits, random_rots)\n",
        "\n",
        "    # Prepare parametrized V\n",
        "    V_circuit, symbols = v_theta(qubits)\n",
        "    exp_circuit = cirq.Circuit(V_circuit for t in range(n_trotter))\n",
        "\n",
        "    # Convert to `tf.Tensor`\n",
        "    initial_U_tensor = tfq.convert_to_tensor([initial_U])\n",
        "    initial_U_splat = tf.tile(initial_U_tensor, [n_points])\n",
        "\n",
        "    full_circuits = tfq.layers.AddCircuit()(initial_U_splat, append=exp_circuit)\n",
        "    # Replace placeholders in circuits with values from `classical_source`.\n",
        "    return tfq.resolve_parameters(\n",
        "        full_circuits, tf.convert_to_tensor([str(x) for x in symbols]),\n",
        "        tf.convert_to_tensor(classical_source * (n_qubits / 3) / n_trotter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNliqKFdYacD"
      },
      "source": [
        "Choose some qubits and prepare the data encoding circuits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F47SaRERKx_"
      },
      "outputs": [],
      "source": [
        "qubits = cirq.GridQubit.rect(1, DATASET_DIM + 1)\n",
        "q_x_train_circuits = prepare_pqk_circuits(qubits, x_train)\n",
        "q_x_test_circuits = prepare_pqk_circuits(qubits, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD1ojMb5PbOG"
      },
      "source": [
        "Next, compute the PQK features based on the 1-RDM of the dataset circuits above and store the results in `rdm`, a `tf.Tensor` with shape `[n_points, n_qubits, 3]`. The entries in `rdm[i][j][k]` = $\\langle \\psi_i | OP^k_j | \\psi_i \\rangle$ where `i` indexes over datapoints, `j` indexes over qubits and `k` indexes over $\\lbrace \\hat{X}, \\hat{Y}, \\hat{Z} \\rbrace$ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEGko5t-SZ14"
      },
      "outputs": [],
      "source": [
        "def get_pqk_features(qubits, data_batch):\n",
        "    \"\"\"Get PQK features based on above construction.\"\"\"\n",
        "    ops = [[cirq.X(q), cirq.Y(q), cirq.Z(q)] for q in qubits]\n",
        "    ops_tensor = tf.expand_dims(tf.reshape(tfq.convert_to_tensor(ops), -1), 0)\n",
        "    batch_dim = tf.gather(tf.shape(data_batch), 0)\n",
        "    ops_splat = tf.tile(ops_tensor, [batch_dim, 1])\n",
        "    exp_vals = tfq.layers.Expectation()(data_batch, operators=ops_splat)\n",
        "    rdm = tf.reshape(exp_vals, [batch_dim, len(qubits), -1])\n",
        "    return rdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZOEdNMzS8hW"
      },
      "outputs": [],
      "source": [
        "x_train_pqk = get_pqk_features(qubits, q_x_train_circuits)\n",
        "x_test_pqk = get_pqk_features(qubits, q_x_test_circuits)\n",
        "print('New PQK training dataset has shape:', x_train_pqk.shape)\n",
        "print('New PQK testing dataset has shape:', x_test_pqk.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tNBzIxT__6"
      },
      "source": [
        "### 2.2 Re-labeling based on PQK features\n",
        "Now that you have these quantum generated features in `x_train_pqk` and `x_test_pqk`, it is time to re-label the dataset. To achieve maximum seperation between quantum and classical performance you can re-label the dataset based on the spectrum information found in `x_train_pqk` and `x_test_pqk`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFSRWagZMTTn"
      },
      "source": [
        "Note: This preparation of your dataset to explicitly maximize the seperation in performance between the classical and quantum models might feel like cheating, but it provides a **very** important proof of existance for datasets that are hard for classical computers and easy for quantum computers to model. There would be no point in searching for quantum advantage in QML if you couldn't first create something like this to demonstrate advantage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLyGksxvGINl"
      },
      "outputs": [],
      "source": [
        "def compute_kernel_matrix(vecs, gamma):\n",
        "    \"\"\"Computes d[i][j] = e^ -gamma * (vecs[i] - vecs[j]) ** 2 \"\"\"\n",
        "    scaled_gamma = gamma / (tf.cast(tf.gather(tf.shape(vecs), 1), tf.float32) *\n",
        "                            tf.math.reduce_std(vecs))\n",
        "    return scaled_gamma * tf.einsum('ijk->ij', (vecs[:, None, :] - vecs)**2)\n",
        "\n",
        "\n",
        "def get_spectrum(datapoints, gamma=1.0):\n",
        "    \"\"\"Compute the eigenvalues and eigenvectors of the kernel of datapoints.\"\"\"\n",
        "    KC_qs = compute_kernel_matrix(datapoints, gamma)\n",
        "    S, V = tf.linalg.eigh(KC_qs)\n",
        "    S = tf.math.abs(S)\n",
        "    return S, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4AxcKa4RRJr"
      },
      "outputs": [],
      "source": [
        "S_pqk, V_pqk = get_spectrum(\n",
        "    tf.reshape(tf.concat([x_train_pqk, x_test_pqk], 0),\n",
        "               [-1, len(qubits) * 3]))\n",
        "\n",
        "S_original, V_original = get_spectrum(tf.cast(tf.concat([x_train, x_test], 0),\n",
        "                                              tf.float32),\n",
        "                                      gamma=0.005)\n",
        "\n",
        "print('Eigenvectors of pqk kernel matrix:', V_pqk)\n",
        "print('Eigenvectors of original kernel matrix:', V_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oULyGmcWC9"
      },
      "source": [
        "Now you have everything you need to re-label the dataset! Now you can consult with the flowchart to better understand how to maximize performance seperation when re-labeling the dataset:\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/quantum/blob/master/docs/tutorials/images/quantum_data_1.png?raw=1\">\n",
        "\n",
        "In order to maximize the seperation between quantum and classical models, you will attempt to maximize the geometric difference between the original dataset and the PQK features kernel matrices $g(K_1 || K_2) = \\sqrt{ || \\sqrt{K_2} K_1^{-1} \\sqrt{K_2} || _\\infty}$ using `S_pqk, V_pqk` and `S_original, V_original`. A large value of $g$ ensures that you initially move to the right in the flowchart down towards a prediction advantage in the quantum case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5EYKcClaHLP"
      },
      "source": [
        "Note: Computing quantities for $s$ and $d$ are also very useful when looking to better understand performance seperations. In this case ensuring a large $g$ value is enough to see performance seperation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-D_939PZoOH"
      },
      "outputs": [],
      "source": [
        "def get_stilted_dataset(S, V, S_2, V_2, lambdav=1.1):\n",
        "    \"\"\"Prepare new labels that maximize geometric distance between kernels.\"\"\"\n",
        "    S_diag = tf.linalg.diag(S**0.5)\n",
        "    S_2_diag = tf.linalg.diag(S_2 / (S_2 + lambdav)**2)\n",
        "    scaling = S_diag @ tf.transpose(V) @ \\\n",
        "              V_2 @ S_2_diag @ tf.transpose(V_2) @ \\\n",
        "              V @ S_diag\n",
        "\n",
        "    # Generate new lables using the largest eigenvector.\n",
        "    _, vecs = tf.linalg.eig(scaling)\n",
        "    new_labels = tf.math.real(\n",
        "        tf.einsum('ij,j->i', tf.cast(V @ S_diag, tf.complex64),\n",
        "                  vecs[-1])).numpy()\n",
        "    # Create new labels and add some small amount of noise.\n",
        "    final_y = new_labels > np.median(new_labels)\n",
        "    noisy_y = (final_y ^ (np.random.uniform(size=final_y.shape) > 0.95))\n",
        "    return noisy_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IkuiFmZRUby"
      },
      "outputs": [],
      "source": [
        "y_relabel = get_stilted_dataset(S_pqk, V_pqk, S_original, V_original)\n",
        "y_train_new, y_test_new = y_relabel[:N_TRAIN], y_relabel[N_TRAIN:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NcCVfLGKsU9"
      },
      "source": [
        "## 3. Comparing models\n",
        "Now that you have prepared your dataset it is time to compare model performance. You will create two small feedforward neural networks and compare performance when they are given access to the PQK features found in `x_train_pqk`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqfjpBOZWmar"
      },
      "source": [
        "### 3.1 Create PQK enhanced model\n",
        "Using standard `tf.keras` library features you can now create and a train a model on the `x_train_pqk` and `y_train_new` datapoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK94tGyf--q2"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "def create_pqk_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(32,\n",
        "                              activation='sigmoid',\n",
        "                              input_shape=[\n",
        "                                  len(qubits) * 3,\n",
        "                              ]))\n",
        "    model.add(tf.keras.layers.Dense(16, activation='sigmoid'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "pqk_model = create_pqk_model()\n",
        "pqk_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "pqk_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUL8ygMn_zOB"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "pqk_history = pqk_model.fit(tf.reshape(x_train_pqk, [N_TRAIN, -1]),\n",
        "                            y_train_new,\n",
        "                            batch_size=32,\n",
        "                            epochs=1000,\n",
        "                            verbose=0,\n",
        "                            validation_data=(tf.reshape(x_test_pqk,\n",
        "                                                        [N_TEST, -1]),\n",
        "                                             y_test_new))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN4Wqa-iLri9"
      },
      "source": [
        "### 3.2 Create a classical model\n",
        "Similar to the code above you can now also create a classical model that doesn't have access to the PQK features in your stilted dataset. This model can be trained using `x_train` and `y_label_new`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHhUYWVh9kGE"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "def create_fair_classical_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(32,\n",
        "                              activation='sigmoid',\n",
        "                              input_shape=[\n",
        "                                  DATASET_DIM,\n",
        "                              ]))\n",
        "    model.add(tf.keras.layers.Dense(16, activation='sigmoid'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_fair_classical_model()\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.03),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N54jMau-1L5"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "classical_history = model.fit(x_train,\n",
        "                              y_train_new,\n",
        "                              batch_size=32,\n",
        "                              epochs=1000,\n",
        "                              verbose=0,\n",
        "                              validation_data=(x_test, y_test_new))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzhs1_CjL_f8"
      },
      "source": [
        "### 3.3 Compare performance\n",
        "Now that you have trained the two models you can quickly plot the performance gaps in the validation data between the two. Typically both models will achieve > 0.9 accuaracy on the training data. However on the validation data it becomes clear that only the information found in the PQK features is enough to make the model generalize well to unseen instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9CDiHTmAEu-"
      },
      "outputs": [],
      "source": [
        "#docs_infra: no_execute\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(classical_history.history['accuracy'], label='accuracy_classical')\n",
        "plt.plot(classical_history.history['val_accuracy'],\n",
        "         label='val_accuracy_classical')\n",
        "plt.plot(pqk_history.history['accuracy'], label='accuracy_quantum')\n",
        "plt.plot(pqk_history.history['val_accuracy'], label='val_accuracy_quantum')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h9p44uCMzHQ"
      },
      "source": [
        "Success: You have engineered a stilted quantum dataset that can intentionally defeat classical models in a fair (but contrived) setting. Try comparing results using other types of classical models. The next step is to try and see if you can find new and interesting datasets that can defeat classical models without needing to engineer them yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvzAMESJaHLU"
      },
      "source": [
        "## 4. Important conclusions\n",
        "\n",
        "There are several important conclusions you can draw from this and the [MNIST](https://www.tensorflow.org/quantum/tutorials/mnist) experiments:\n",
        "\n",
        "1. It's very unlikely that the quantum models of today will beat classical model performance on classical data. Especially on today's classical datasets that can have upwards of a million datapoints.\n",
        "\n",
        "2. Just because the data might come from a hard to classically simulate quantum circuit, doesn't necessarily make the data hard to learn for a classical model.\n",
        "\n",
        "3. Datasets (ultimately quantum in nature) that are easy for quantum models to learn and hard for classical models to learn do exist, regardless of model architecture or training algorithms used."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "quantum_data.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}